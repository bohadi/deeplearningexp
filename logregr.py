# -*- coding: utf-8 -*-
"""logregr.ipynb

Automatically generated by Colaboratory.

Original file is located at
"""

# Commented out IPython magic to ensure Python compatibility.
'''
binary logistic regression MNIST
'''

# %tensorflow_version 2.x
import numpy as np
import matplotlib.pyplot as plt
# dont forget to reupload mnist_data_loader.py on each new colab runtime
import mnist_data_loader

# prediction step
def sigmoid(X, W, b=0):
  z = np.dot(X, W) + b
  return 1 / (1 + np.exp(-z))

# count the errors
def classify(Z, Y):
  YHAT = np.round(Z)
  return np.count_nonzero(YHAT!=Y)

# 2class cross-entropy
def log_loss(Z, Y):
  return -(Y*np.log(Z) + (1 - Y)*np.log(1 - Z)).mean()

# gradient of loss wrt W
def gradient(X, Z, Y, batch_size):
  return np.dot((Z - Y), X) / batch_size

# update weights w regularization
def update(W, grad, alpha):
  return W - alpha * grad

# load data
mnist_dataset = mnist_data_loader.read_data_sets("./MNIST_data/", one_hot=False)
train_set = mnist_dataset.train
test_set = mnist_dataset.test
image = train_set.images[0]
print('\nTraining dataset size: ', train_set.num_examples)
# hyperparams
batch_size = 100
max_epoch = 100
learning_rate = 1e-1
# initialize W+b 
W = np.zeros(784)
b = 0
# training
print('Training starting ||W||:', np.linalg.norm(W))
accuracy = []
for epoch in range(0, max_epoch):
  iter_per_batch = train_set.num_examples // batch_size
  errors = 0
  for batch_id in range(0, iter_per_batch):
    batch = train_set.next_batch(batch_size)
    X, label = batch
    Y = label/3 - 1
    # prediction, accuracy, logloss, gradient, update
    Z = sigmoid(X, W, b)
    errors += classify(Z, Y)
    loss = log_loss(Z, Y)
    dedw = gradient(X, Z, Y, batch_size)
    W = update(W, dedw, learning_rate)
    if epoch == 0 and (batch_id < 10 or batch_id % 20 == 0):
      print('\tEpoch 0 Minibatch:', batch_id, 'Accuracy:', 1-(errors/(batch_size*(batch_id+1))))
  accuracy.append(1 - (errors / (iter_per_batch*batch_size)))
  if epoch % 10 == 0:
    print('\tEpoch', epoch, 'Accuracy:', 1-accuracy[-1])
print('Training ending ||W||:', np.linalg.norm(W))
plt.plot(accuracy)
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.show()
print('\nTesting model')
# test set
X,lbl = test_set.next_batch(test_set.num_examples)
Y = lbl/3-1
Z = sigmoid(X, W, b)
# count and show incorrect examples
err_idx = [i for (i,y) in enumerate(Y) if y!=np.round(Z)[i]]
for i in err_idx[-3:-1]:
  print('Example #', i, 'Expected', test_set.labels[i], 'Predicted', 3+3*int(np.round(Z)[i]))
  plt.imshow(np.reshape(test_set.images[i],[28,28]),cmap='gray')
  plt.show()
print('\nTest set error rate: ', len(err_idx), '/', test_set.num_examples, 'Accuracy:', 1-len(err_idx)/test_set.num_examples)
